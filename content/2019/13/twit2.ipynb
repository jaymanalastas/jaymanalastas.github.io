{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Word Frequency Counts\n",
    "\n",
    "This section of the blog will deal with the data we pull from Twitter. We'll clean up the data and figure out the count of words found in tweets in tandem with our search query.\n",
    "\n",
    "First, we download all libraries will need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jaymanalastas_1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as py\n",
    "import seaborn as sns \n",
    "import itertools \n",
    "import collections\n",
    "import config\n",
    "import string\n",
    "\n",
    "import tweepy as tw\n",
    "import nltk\n",
    "# from nltk.cropus import stopwords\n",
    "import re\n",
    "import networkx #library for creation, manipulation, and study of the structure, \n",
    "#dynamics, and functions of complex networks. \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did before, we have to define our api keys:\n",
    " - consumer_key= 'yourkeyhere'\n",
    " - consumer_secret= 'yourkeyhere'\n",
    " - access_token= 'yourkeyhere'\n",
    " - access_token_secret= 'yourkeyhere'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(config.consumer_key, config.consumer_secret)\n",
    "auth.set_access_token(config.access_token, config.access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have authenticated, we'll set up our query again from part 1 and get our data. This time we'll use tesla as a search term and dig through that data to find insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#PyTorch in action at #Tesla :) https://t.co/c8h9HJI8V2',\n",
       " \"@polizeiberlin Don't ridicule this poor #Ford owner, who probably just has an uncontrollable desire to be driving… https://t.co/h0q7uA7g5f\",\n",
       " '@TotalGamix @Model3Owners @vincent13031925 @28delayslater @TeslaJoy @tesla_raj @gwestr Haha I like your positive at… https://t.co/CsCK7ATJAC',\n",
       " '@Kristennetten @Sofiaan @elonmusk Think sooner for sure esp. since #Autopilot was made standard :) #Tesla for the win',\n",
       " \"#tesla @elonmusk Bwahahahha Tesla stock shots up on the announcement that they're going to announce the product the… https://t.co/WaTECfQC4x\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sea_tsla = '#tesla :)' + \"-filter:retweets\"\n",
    "\n",
    "tweets = tw.Cursor(api.search, \n",
    "                  q=sea_tsla,\n",
    "                  lang='en',\n",
    "                  since='2019-08-01'). items(1000)\n",
    "\n",
    "all_tweets = [tweet.text for tweet in tweets]\n",
    "\n",
    "all_tweets[:5]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a function that'll remove urls called __remove_url__ and another one that'll clean our text of punctuations, stopwords and capital letters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(txt):\n",
    "    \"\"\"\n",
    "    Replace URLs found in a text string nothing\n",
    "    \n",
    "    Parameters\n",
    "    -------------------------------------------------------------------\n",
    "    txt: string \n",
    "        A text string that you want to parse and remove urls.\n",
    "        \n",
    "    Returns\n",
    "    -------------------------------------------------------------------\n",
    "    The same txt string with url's removed. \n",
    "    \"\"\"\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(txt):\n",
    "    \"\"\"\"\n",
    "    Removes the following: \n",
    "            Punctuations\n",
    "            Stop words\n",
    "    Return list of clean text words\n",
    "    \n",
    "    Parameters\n",
    "    -------------------------------------------------------------------\n",
    "    txt: string \n",
    "        A text string that you want to parse \n",
    "        \n",
    "    Returns\n",
    "    -------------------------------------------------------------------\n",
    "    Clean words \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    nonpunc = [char for char in txt if char not in string.punctuation]\n",
    "    nonpunc = ''.join(nonpunc)\n",
    "    return [word for word in nonpunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_clean = [text_process(tweet) for tweet in all_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_no_url = [remove_url(tweet) for tweet in all_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits: \n",
    "\n",
    " - https://www.python-course.eu/networkx.php"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
